{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NCzJkEM6DLd",
        "outputId": "82034179-bba1-4833-f08e-ef15af3b775d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/gdrive/My Drive/Datasets/BaiThi2.zip')\n",
        "zip_ref.extractall()"
      ],
      "metadata": {
        "id": "zLJxh27o7E-y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "AAfs9-nF7qPB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_spam = os.listdir(\"/content/TrainData/spam/\")\n",
        "data_not_spam = os.listdir(\"/content/TrainData/notspam/\")\n"
      ],
      "metadata": {
        "id": "_nMg2vLc7TCI"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_spam = []\n",
        "label_spam = []\n",
        "for i in data_spam:\n",
        "  path = os.path.join(\"/content/TrainData/spam/\", i)\n",
        "  with open(path, 'r') as f:\n",
        "    dt_spam.append(f.readlines())\n",
        "    label_spam.append(int(0))\n",
        "len(dt_spam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pXVZdHm_Ay9",
        "outputId": "76ce62c9-dea5-4889-a878-96ac3449733c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_not_spam = []\n",
        "label_not_spam = []\n",
        "for i in data_not_spam:\n",
        "  path = os.path.join(\"/content/TrainData/notspam/\", i)\n",
        "  with open(path, 'r') as f:\n",
        "    dt_not_spam.append(f.readlines())\n",
        "    label_not_spam.append(int(1))\n",
        "len(dt_not_spam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zg_O790ACil",
        "outputId": "7586a9e6-b8fe-4523-f291-51977b1181e3"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "193"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "data_train = dt_spam[:12]+dt_not_spam[:150]\n",
        "random.seed(0)\n",
        "random.shuffle(data_train)\n",
        "\n",
        "label_train = label_spam[:12]+label_not_spam[:150]\n",
        "random.seed(0)\n",
        "random.shuffle(label_train)\n",
        "\n",
        "\n",
        "data_test = dt_spam[12:]+dt_not_spam[150:]\n",
        "label_test = label_spam[12:]+label_not_spam[150:]\n",
        "print(len(data_train))\n",
        "print(len(data_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYM7tvAaCKU6",
        "outputId": "f7a0444c-ea2c-443d-fba5-bff4da501433"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162\n",
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text processing"
      ],
      "metadata": {
        "id": "Slvt1dOc9FGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS"
      ],
      "metadata": {
        "id": "4DtxcR_Z9H8r"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the raw data\n",
        "def remove_hyperlink(word):\n",
        "    return  re.sub(r\"http\\S+\", \"\", word)\n",
        "\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result\n",
        "\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', '', word)\n",
        "    return result\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return result\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "    s = result.split()\n",
        "    return (' '.join (str (x) for x in s))\n",
        "    \n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n','')\n",
        "\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink,\n",
        "                      replace_newline,\n",
        "                      to_lower,\n",
        "                      remove_number,\n",
        "                      remove_punctuation,remove_whitespace]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "Fxo5VFkY9C0b"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [clean_up_pipeline(str(x)) for x in data_train]\n",
        "X_test = [clean_up_pipeline(str(x)) for x in data_test]\n",
        "\n",
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "r9iR0YDP9LJo",
        "outputId": "c6e00dd7-4126-4ae0-d1e7-0338f6c66737"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'subject re language acquisition analogy pragmaticsn n yes indeed careful star sentence sentencing star does meaning passerby one evening brentwood several weeks ago witnessed cruel double murder testimony broken english required services professional linguist interpret latter was nt sure once o j simpson starred sentence result is future history jules levin n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tokenize"
      ],
      "metadata": {
        "id": "ynnu2SYm9Fop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "x_train_features = np.array(tokenizer.texts_to_sequences(X_train))\n",
        "x_test_features = np.array(tokenizer.texts_to_sequences(X_test))\n",
        "\n",
        "len(x_train_features[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx1EhcP_9MDy",
        "outputId": "11c6bec5-5fb0-4cf4-dc11-595b0a046816"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Text               : \",X_train[i] )\n",
        "    print(\"Numerical Sequence : \", x_train_features[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBcpAEvvuUXT",
        "outputId": "e33ec800-b7ec-4b18-fd54-cbd205a3eb11"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text               :  subject re language acquisition analogy pragmaticsn n yes indeed careful star sentence sentencing star does meaning passerby one evening brentwood several weeks ago witnessed cruel double murder testimony broken english required services professional linguist interpret latter was nt sure once o j simpson starred sentence result is future history jules levin n\n",
            "Numerical Sequence :  [7, 53, 6, 164, 1447, 2590, 2, 319, 962, 2591, 2592, 621, 4072, 2592, 33, 172, 4073, 11, 1160, 4074, 320, 401, 259, 4075, 4076, 708, 4077, 4078, 4079, 12, 2593, 709, 495, 70, 4080, 963, 10, 27, 213, 357, 173, 120, 4081, 4082, 621, 1161, 1, 710, 496, 2594, 4083, 2]\n",
            "Text               :  subject human sense disambiguation correctionn n oh dear first send summary late discover summary has error second reference ve read g miller communication annual review psychology vol pages sorry mail mark sanderson department computing science university glasgow g qq scotland uk e mail sanderso dcs glasgow ac uk tel x number fax m gonna tent tent tent tent tent both experiment ment ment ment ment n\n",
            "Numerical Sequence :  [7, 138, 287, 2595, 4084, 2, 4085, 449, 47, 35, 288, 817, 818, 288, 13, 1448, 112, 289, 241, 321, 115, 1860, 1162, 2596, 711, 964, 551, 1861, 1163, 62, 402, 4086, 24, 1449, 65, 8, 965, 115, 2597, 819, 100, 16, 62, 4087, 4088, 965, 151, 100, 322, 966, 36, 44, 19, 4089, 1164, 1164, 1164, 1164, 1164, 59, 1862, 1450, 1450, 1450, 1450, 2]\n",
            "Text               :  subject summary doctoral programs applied linguisticsn n several weeks ago posted query doctoral programs applied linguistics received responses thanks respondents responses are organized below note were responses georgetown ucla though were mentioned one response michael toolan toolan u washington edu nt entirely separate program applied linguistics within our english dept provide concentration courses language discourse studies leading ph d re particularly equipped supervise dissertations rhetorical composition theory b discourse analysis among students taking options are ones completed our ma tesol michael toolan dept english u washington seattle robert port port cs indiana edu depends means applied linguistics indiana university has program area is focussed especially teaching english second language offers training topics lexicography creoles sociolinguistics etc wants phd linguistics request information department linguistics memorial hall indiana university bloomington lingdept indiana edu stanley dubinsky dubinsk univscvm csd scarolina edu greetings friend might want consider applying linguistics program here university south carolina linguistics programhere is interdepartmental program involving core faculty seven depts including english french spanish anthro phil psych speech path graduate students most whom are specializing sla theory esl popular specializations here include general sociolinguistics discourse conversation analysis codeswitching dialect studies are three things might program attractive friend besides winter weather lack thereof involvement faculty diverse departments creates wealth potential avenues research our students ii despite interdisciplinary nature program requirements strong emphasis core areas theoretical linguistics iii hired sla theoretician program has questions pleased answer best stan dubinsky stanley dubinsky e mail dubinsk univscvm csd scarolina edu linguistics program phone u south carolina fax columbia sc one anonymous respondent suggested uc berkeley has used flexible system whereby student create own program person suggested penn state educational linguistics university hawai mano sarah g thomason sally isp pitt edu pitt has specialty applied linguistics within ph d program linguistics students little interest linguistics per se are better off applied linguistics ph d program are quite few ucla hawaii most prominent carnegie mellon university announced one faculty is small financial aid is likely gather nativelike fluency french german spanish japanese usc southern calif has ph d program applied ling wrong maybe s ours linguistics specialization track app ling likewise u delaware sally alan juffs juffs isp pitt edu university pittsburgh offers phd applied linguistics believe is strong program since gives students thorough training linguistics opportunity actually work applied areas students are currently funded through english language institute various research projects relate language teaching applied program is particularly strong field second language acquisition both ug oriented cognitive approaches addition offer courses language planning sociolinguistics languages contact please nt hesitate write further information alan juffs admissions officer alan juffs tel dept linguistics fax cl email juffs isp pitt edu university pittsburgh pittsburgh pa usa marina mcintire mmcintir lynx dac neu edu program bu applied linguistics is pretty one one thing s asl connection is strong one s pretty far theoretical side still ricky jacobs hi susie sic voice past program here is direct counterpart applied program ucla is called political reasons ph d sla though covers much s housed esl here has quite few recognized stars details check professor charlene sato chair ph d program sla department english second language university hawai manoa east west road honolulu hi ali aghbar aaghbar grove iup edu ph d program rhetoric linguistics students choose concentrations composition tesol enroll round summers information contact director graduate programs rhetoric linguistics department english indiana university pa indiana pa susan fischer internet sdfncr rit edu national technical institute deaf phone rochester institute technology fax lomb memorial drive basic food groups popcorn rochester ny tofu bok choy chocolaten\n",
            "Numerical Sequence :  [7, 288, 1863, 260, 94, 1165, 2, 320, 401, 259, 820, 121, 1863, 260, 94, 9, 214, 122, 85, 967, 122, 4, 968, 195, 242, 71, 122, 4090, 403, 323, 71, 497, 11, 821, 290, 1864, 1864, 63, 622, 32, 27, 2598, 1166, 38, 94, 9, 152, 17, 12, 324, 325, 4091, 196, 6, 197, 101, 2599, 261, 22, 53, 358, 4092, 1865, 2600, 4093, 1167, 54, 86, 197, 88, 498, 72, 712, 1866, 4, 1867, 713, 17, 623, 1451, 290, 1864, 324, 12, 63, 622, 1868, 1168, 2601, 2601, 1169, 714, 32, 2602, 291, 94, 9, 714, 8, 13, 38, 262, 1, 4094, 326, 128, 12, 112, 6, 715, 1869, 1870, 2603, 4095, 716, 76, 717, 624, 9, 1170, 15, 24, 9, 1871, 969, 714, 8, 4096, 4097, 714, 32, 2604, 1872, 2605, 1873, 1874, 1875, 32, 2606, 625, 102, 113, 970, 4098, 9, 38, 49, 8, 822, 1876, 9, 4099, 1, 1877, 38, 1878, 1452, 243, 2607, 2608, 139, 12, 67, 198, 2609, 1453, 2610, 107, 4100, 263, 72, 55, 1879, 4, 4101, 1454, 54, 971, 4102, 4103, 49, 327, 103, 716, 197, 4104, 88, 4105, 226, 101, 4, 215, 264, 102, 38, 1880, 625, 1455, 1171, 4106, 2611, 2612, 4107, 243, 2613, 450, 4108, 2614, 499, 2615, 57, 17, 72, 972, 2616, 2617, 973, 38, 626, 500, 1881, 1452, 328, 359, 9, 2618, 2619, 1454, 4109, 38, 13, 216, 2620, 501, 502, 2621, 1872, 2604, 1872, 16, 62, 2605, 1873, 1874, 1875, 32, 9, 38, 174, 63, 822, 1876, 44, 4110, 4111, 11, 1172, 974, 360, 1882, 975, 13, 73, 4112, 329, 4113, 129, 823, 89, 38, 217, 360, 4114, 123, 2622, 9, 8, 1883, 4115, 1884, 115, 4116, 2623, 718, 1456, 32, 1456, 13, 4117, 94, 9, 152, 261, 22, 38, 9, 72, 41, 153, 9, 227, 552, 4, 976, 977, 94, 9, 261, 22, 38, 4, 244, 140, 403, 2624, 55, 2625, 4118, 4119, 8, 1457, 11, 243, 1, 330, 824, 1173, 1, 1885, 2626, 4120, 1886, 67, 331, 198, 978, 1458, 627, 4121, 13, 261, 22, 38, 94, 825, 979, 719, 3, 4122, 9, 2627, 1174, 1887, 825, 1888, 63, 4123, 2623, 980, 1175, 1175, 718, 1456, 32, 8, 1889, 715, 624, 94, 9, 451, 1, 500, 38, 87, 981, 72, 2628, 1869, 9, 245, 628, 48, 94, 328, 72, 4, 503, 2629, 141, 12, 6, 504, 246, 57, 826, 2630, 6, 128, 94, 38, 1, 358, 500, 265, 112, 6, 164, 59, 4124, 1176, 553, 982, 404, 629, 196, 6, 1459, 716, 14, 175, 31, 27, 2631, 827, 116, 15, 980, 1175, 4125, 2632, 980, 1175, 322, 324, 9, 44, 4126, 18, 1175, 718, 1456, 32, 8, 1889, 1889, 720, 452, 4127, 4128, 4129, 2633, 4130, 2634, 32, 38, 1890, 94, 9, 1, 1177, 11, 11, 228, 3, 1891, 554, 1, 500, 11, 3, 1177, 229, 359, 721, 142, 4131, 4132, 1178, 4133, 4134, 1179, 405, 38, 49, 1, 828, 4135, 94, 38, 403, 1, 453, 406, 555, 261, 22, 1454, 323, 2635, 60, 3, 4136, 971, 49, 13, 244, 140, 2636, 4137, 556, 505, 506, 4138, 4139, 630, 261, 22, 38, 1454, 24, 12, 112, 6, 8, 1883, 4140, 1180, 983, 1460, 2637, 1178, 1892, 4141, 4142, 4143, 4144, 32, 261, 22, 38, 1181, 9, 72, 1461, 4145, 1167, 1451, 4146, 1893, 4147, 15, 175, 984, 263, 260, 1181, 9, 24, 12, 714, 8, 720, 714, 720, 2638, 4148, 185, 4149, 4150, 32, 454, 292, 504, 2639, 174, 1462, 504, 722, 44, 4151, 1871, 985, 332, 1894, 829, 4152, 1462, 986, 4153, 4154, 4155, 4156]\n",
            "Text               :  subject oulipon n am looking information oulipo experimental language group flourished s paris members included r queneau h matthews calvino g perec group applied mathematical methods writing literature is scant information available english small amount french am particularly interested whether ideas been embodied software literaty theory thank michael sikillian annotextn\n",
            "Numerical Sequence :  [7, 4157, 2, 23, 199, 15, 4158, 2640, 6, 361, 4159, 3, 1463, 165, 1464, 90, 4160, 154, 4161, 4162, 115, 4163, 361, 94, 4164, 1465, 293, 166, 1, 4165, 15, 95, 12, 330, 723, 67, 23, 358, 117, 82, 557, 26, 4166, 186, 4167, 54, 362, 290, 2641, 2642]\n",
            "Text               :  subject re political correctnessn n term politically correct together political correctness pc are being used increasing frequency did nexis search couple weeks ago connection presentation m doing ncte next week shift meaning term while pc still retains negative leftwing ironic slant is coming used broadly attack partyline extremists stripe broadly still often without apparent negative ironic connotation refer doing right thing expected thing appro priate thing context example appears business week info world nonpolitical contexts articles salary levels corporate ceos appropriate values return writing line computer programming code one nice cite noted while was easy send email boss clerk mailroom was necessarily politically correct wonder often send email mailroom clerks dennis debaron uiuc edu fax dennis baron department english univ illinois s wright st urbana il n\n",
            "Numerical Sequence :  [7, 53, 406, 4168, 2, 104, 187, 130, 407, 406, 1466, 631, 4, 96, 73, 2643, 987, 218, 4169, 988, 1467, 401, 259, 554, 2644, 19, 558, 4170, 219, 167, 1895, 172, 104, 200, 631, 142, 4171, 507, 2645, 989, 4172, 1, 1896, 73, 1897, 2646, 4173, 4174, 4175, 1897, 142, 559, 188, 2647, 507, 989, 2648, 990, 558, 201, 228, 991, 228, 4176, 4177, 228, 632, 266, 1468, 143, 167, 633, 97, 4178, 1469, 1898, 634, 635, 4179, 4180, 636, 4181, 1182, 293, 560, 363, 1470, 561, 11, 1899, 2649, 1471, 200, 10, 408, 35, 18, 2650, 4182, 2651, 10, 1183, 187, 130, 1900, 559, 35, 18, 2651, 4183, 2652, 4184, 1901, 32, 44, 2652, 4185, 24, 12, 2653, 1184, 3, 1472, 455, 2654, 1473, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.index_word"
      ],
      "metadata": {
        "id": "OW6vS2XnpD3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(label_train)\n",
        "y_test = le.transform(label_test)\n",
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBC8HD5ZrREf",
        "outputId": "d9a76c2e-84ff-4ecf-ffbb-77fb2f0fbbc2"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 20 # how big is each word vector\n",
        "max_feature = len(tokenizer.word_index)+1\n",
        "max_len = max_length_sequence = max([len(i) for i in x_train_features])\n",
        "print(\"max_feature\", max_feature)\n",
        "print(\"max_len\", max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKDk6HY8tzZf",
        "outputId": "125296f8-26d8-4e98-a371-ef65fdb3e573"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_feature 8895\n",
            "max_len 2277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "x_train_features = pad_sequences(x_train_features,maxlen=max_len)\n",
        "x_test_features = pad_sequences(x_test_features,maxlen=max_len)\n",
        "x_train_features[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMoQTyA_u_sL",
        "outputId": "3a966c42-8cca-47e7-d838-3373c265f55a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0, ..., 2594, 4083,    2], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Model"
      ],
      "metadata": {
        "id": "be8m6TtN9Y0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional\n",
        "from keras.models import Model\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "lycZmWxal4me"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    \n",
        "      lstm_model = tf.keras.Sequential()\n",
        "      lstm_model.add(Embedding(max_feature, 32, input_length=max_len))\n",
        "      lstm_model.add(LSTM(64))\n",
        "      lstm_model.add(Dropout(0.5))\n",
        "      lstm_model.add(Dense(20, activation=\"relu\"))\n",
        "      lstm_model.add(Dropout(0.4))\n",
        "      lstm_model.add(Dense(1, activation = \"sigmoid\"))\n",
        "      return lstm_model\n",
        "\n",
        "lstm_model = create_model()\n",
        "lstm_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8iATSKIjtrx",
        "outputId": "d6fa3149-89eb-419e-d164-c9fdd4c773ae"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 2277, 32)          284640    \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 64)                24832     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 20)                1300      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 20)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 310,793\n",
            "Trainable params: 310,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "his = lstm_model.fit(x_train_features, y_train, batch_size=16, epochs=14, validation_data=(x_test_features, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOD8JT-Yqrv3",
        "outputId": "2d4624ab-fc9c-43ef-d8c2-f8349710cb72"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/14\n",
            "11/11 [==============================] - 3s 133ms/step - loss: 0.6657 - accuracy: 0.8395 - val_loss: 0.6102 - val_accuracy: 0.8776\n",
            "Epoch 2/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.4710 - accuracy: 0.9198 - val_loss: 0.4429 - val_accuracy: 0.8776\n",
            "Epoch 3/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.3204 - accuracy: 0.9198 - val_loss: 0.3860 - val_accuracy: 0.8776\n",
            "Epoch 4/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.3032 - accuracy: 0.9259 - val_loss: 0.3569 - val_accuracy: 0.8776\n",
            "Epoch 5/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.3041 - accuracy: 0.9259 - val_loss: 0.3419 - val_accuracy: 0.8776\n",
            "Epoch 6/14\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.2652 - accuracy: 0.9259 - val_loss: 0.3750 - val_accuracy: 0.8776\n",
            "Epoch 7/14\n",
            "11/11 [==============================] - 1s 84ms/step - loss: 0.2170 - accuracy: 0.9259 - val_loss: 0.2627 - val_accuracy: 0.8776\n",
            "Epoch 8/14\n",
            "11/11 [==============================] - 1s 84ms/step - loss: 0.1492 - accuracy: 0.9630 - val_loss: 0.2263 - val_accuracy: 0.8776\n",
            "Epoch 9/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.0665 - accuracy: 0.9815 - val_loss: 0.2744 - val_accuracy: 0.8776\n",
            "Epoch 10/14\n",
            "11/11 [==============================] - 1s 84ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9184\n",
            "Epoch 11/14\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9388\n",
            "Epoch 12/14\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.1918 - val_accuracy: 0.9184\n",
            "Epoch 13/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9796\n",
            "Epoch 14/14\n",
            "11/11 [==============================] - 1s 81ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict  = [1 if x>=0.5 else 0 for x in lstm_model.predict(x_test_features)]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test,y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3IeSJOQFy1v",
        "outputId": "347a9595-a47a-407a-c2ee-ddd4abdf771d"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 60ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.83      0.91         6\n",
            "           1       0.98      1.00      0.99        43\n",
            "\n",
            "    accuracy                           0.98        49\n",
            "   macro avg       0.99      0.92      0.95        49\n",
            "weighted avg       0.98      0.98      0.98        49\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "label on test"
      ],
      "metadata": {
        "id": "EWt1QffqOl6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_not_spam = os.listdir(\"/content/TestData_nolabel/\")\n",
        "\n",
        "test_nolabel = []\n",
        "\n",
        "for i in data_not_spam:\n",
        "  path = os.path.join(\"/content/TestData_nolabel/\", i)\n",
        "  with open(path, 'r') as f:\n",
        "    test_nolabel.append(f.readlines())\n",
        "len(test_nolabel)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr0t_KO3OhiM",
        "outputId": "f8df65c3-ca9d-4d01-c4e8-cb1cc38c0c61"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_test = [clean_up_pipeline(str(x)) for x in test_nolabel]\n",
        "\n",
        "arr_test[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "y-hWAMhUO81w",
        "outputId": "c416ef4e-14e0-4503-87f7-fa02a767b18e"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'subject black hole pre chomskyan citationsn n steve anderson s book morphology contain follow ing epigraph did nt change before publication linguistics become science linguists begin standing one another s shoulders instead one another s toes has point reached point where are redoing aspects language poorly were done first problem originated fact little had been done syntax prior work generative school little is comparison has been done since instigation movement jakobson s halle s work distinctive features clearly superceded pre vious work making difficult structuralist work relevant is going today however anderson is right chiding us carrying attitude over morphology where current trend around massachusetts has hardly moved beyond bloomfield first claim affixes are regular lexical items first rate morphologi cal study goes back stoic philosophers were first tease apart grammatical categories different track back panini is most current morphology failing cite relevant sources is failing advantage discoveries struc turalist neogrammarian even classical morphologists pre decessors were particularly adept finding problems theory linguistic sign varro was first attempt define lexical categories terms n v lexicalizations aristotle noticed grammatical morphemes differed lexical ones stoics first used terms signifier signified am jumping middle discussion mark has touched real issue is less chomsky members school are quoted much many others contri butions often same ones are quoted little result am seeing often is second third fourth reinvention wheel rbeardn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# tokenizer = Tokenizer(num_words=max_feature)\n",
        "\n",
        "# tokenizer.fit_on_texts(arr_test)\n",
        "\n",
        "arr_test_feature = np.array(tokenizer.texts_to_sequences(arr_test))\n",
        "\n",
        "arr_test_feature[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgsMdQQvPctv",
        "outputId": "9f8aa75f-bb9e-4585-ed78-47d844226a55"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7,\n",
              " 53,\n",
              " 469,\n",
              " 4393,\n",
              " 5261,\n",
              " 2,\n",
              " 221,\n",
              " 899,\n",
              " 555,\n",
              " 459,\n",
              " 4,\n",
              " 29,\n",
              " 230,\n",
              " 469,\n",
              " 340,\n",
              " 3,\n",
              " 78,\n",
              " 1212,\n",
              " 265,\n",
              " 1,\n",
              " 313,\n",
              " 2636,\n",
              " 1049,\n",
              " 262,\n",
              " 1613,\n",
              " 3072,\n",
              " 463,\n",
              " 79,\n",
              " 1090,\n",
              " 1044,\n",
              " 140,\n",
              " 205,\n",
              " 259,\n",
              " 506,\n",
              " 9,\n",
              " 463,\n",
              " 601,\n",
              " 2252,\n",
              " 213,\n",
              " 1,\n",
              " 1339,\n",
              " 1478,\n",
              " 3722,\n",
              " 2444,\n",
              " 7091,\n",
              " 78,\n",
              " 899,\n",
              " 4175,\n",
              " 359,\n",
              " 5710,\n",
              " 17,\n",
              " 5524,\n",
              " 2636,\n",
              " 865,\n",
              " 1,\n",
              " 464,\n",
              " 250,\n",
              " 1192,\n",
              " 835,\n",
              " 105,\n",
              " 469,\n",
              " 246,\n",
              " 555,\n",
              " 2823,\n",
              " 36,\n",
              " 77,\n",
              " 2212,\n",
              " 2358,\n",
              " 205,\n",
              " 124,\n",
              " 11,\n",
              " 427,\n",
              " 10,\n",
              " 70,\n",
              " 847,\n",
              " 469,\n",
              " 124,\n",
              " 6074,\n",
              " 178,\n",
              " 667,\n",
              " 469,\n",
              " 10,\n",
              " 11,\n",
              " 250,\n",
              " 496,\n",
              " 60,\n",
              " 976,\n",
              " 821,\n",
              " 96,\n",
              " 847,\n",
              " 29,\n",
              " 14,\n",
              " 1243,\n",
              " 200,\n",
              " 142,\n",
              " 3,\n",
              " 579,\n",
              " 17,\n",
              " 265,\n",
              " 1,\n",
              " 3268,\n",
              " 579,\n",
              " 498,\n",
              " 2941,\n",
              " 748,\n",
              " 352,\n",
              " 2828,\n",
              " 2829]"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "arr_test_feature = pad_sequences(arr_test_feature,maxlen=max_len)\n",
        "\n",
        "arr_test_feature[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWv25_PXPyfV",
        "outputId": "0b11ff58-9ddb-435a-f89c-723ef0bac377"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0, ...,  352, 2828, 2829], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict_label  = [1 if x>=0.5 else 0 for x in lstm_model.predict(arr_test_feature)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49BOXexuOeVu",
        "outputId": "3740ef20-4951-4083-997e-c86a9e3c7a08"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 58ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict_label.count(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60nt5pfLP_Cu",
        "outputId": "b768b339-932d-43da-ff0e-5467a40dd209"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/label_predict.txt', 'w+') as f:\n",
        "     \n",
        "    # write elements of list\n",
        "    for items in y_predict_label:\n",
        "        f.write('%s\\n' %items)\n",
        "     \n",
        "    print(\"File written successfully\")\n",
        " \n",
        " \n",
        "# close the file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5bzUvHh04Fy",
        "outputId": "c652be1d-2ae8-4408-9491-bec7f01e206c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File written successfully\n"
          ]
        }
      ]
    }
  ]
}