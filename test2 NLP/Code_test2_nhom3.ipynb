{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NCzJkEM6DLd",
        "outputId": "fb2858ce-bb06-4d26-9ab1-55471b14683c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zLJxh27o7E-y"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/gdrive/My Drive/Datasets/BaiThi2.zip')\n",
        "zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AAfs9-nF7qPB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_nMg2vLc7TCI"
      },
      "outputs": [],
      "source": [
        "data_spam = os.listdir(\"/content/TrainData/spam/\")\n",
        "data_not_spam = os.listdir(\"/content/TrainData/notspam/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pXVZdHm_Ay9",
        "outputId": "9c3bbad7-11f2-4098-f2ab-3e2163618c3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "dt_spam = []\n",
        "label_spam = []\n",
        "for i in data_spam:\n",
        "  path = os.path.join(\"/content/TrainData/spam/\", i)\n",
        "  with open(path, 'r') as f:\n",
        "    dt_spam.append(f.readlines())\n",
        "    label_spam.append(int(0))\n",
        "len(dt_spam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zg_O790ACil",
        "outputId": "a7ad1a10-5e60-493c-a021-56527b75cd37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "193"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "dt_not_spam = []\n",
        "label_not_spam = []\n",
        "for i in data_not_spam:\n",
        "  path = os.path.join(\"/content/TrainData/notspam/\", i)\n",
        "  with open(path, 'r') as f:\n",
        "    dt_not_spam.append(f.readlines())\n",
        "    label_not_spam.append(int(1))\n",
        "len(dt_not_spam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYM7tvAaCKU6",
        "outputId": "a20e70fd-5284-4a0b-ffdf-59f199d39ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162\n",
            "49\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "data_train = dt_spam[:12]+dt_not_spam[:150]\n",
        "random.seed(0)\n",
        "random.shuffle(data_train)\n",
        "\n",
        "label_train = label_spam[:12]+label_not_spam[:150]\n",
        "random.seed(0)\n",
        "random.shuffle(label_train)\n",
        "\n",
        "\n",
        "data_test = dt_spam[12:]+dt_not_spam[150:]\n",
        "label_test = label_spam[12:]+label_not_spam[150:]\n",
        "print(len(data_train))\n",
        "print(len(data_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slvt1dOc9FGo"
      },
      "source": [
        "text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4DtxcR_Z9H8r"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "WhXTmMBipa5a"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Fxo5VFkY9C0b"
      },
      "outputs": [],
      "source": [
        "#cleaning the raw data\n",
        "def remove_hyperlink(word):\n",
        "    return  re.sub(r\"http\\S+\", \"\", word)\n",
        "\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result\n",
        "\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', '', word)\n",
        "    return result\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return result\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "    s = result.split()\n",
        "    return (' '.join (str (x) for x in s))\n",
        "\n",
        "def skimming(word):\n",
        "  stemmer = PorterStemmer()\n",
        "  singles = [stemmer.stem(x) for x in word.split()]\n",
        "  return(' '.join(singles))\n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n','')\n",
        "\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink,\n",
        "                      replace_newline,\n",
        "                      to_lower,\n",
        "                      remove_number,\n",
        "                      remove_punctuation,remove_whitespace,\n",
        "                      skimming]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_number(\"  con  sa909324   432424 anh yeu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tzqGRv6Ziiqa",
        "outputId": "3baedfd1-0289-4a43-86cb-1e24d31fd900"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  con  sa    anh yeu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove"
      ],
      "metadata": {
        "id": "jmCiObJtkf-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "r9iR0YDP9LJo",
        "outputId": "5832c40b-513a-404b-b743-6bbc9fc14f3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'subject re languag acquisit analog pragmaticsn n ye inde care star sentenc sentenc star doe mean passerbi one even brentwood sever week ago wit cruel doubl murder testimoni broken english requir servic profession linguist interpret latter wa nt sure onc o j simpson star sentenc result is futur histori jule levin n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "X_train = [clean_up_pipeline(str(x)) for x in data_train]\n",
        "X_test = [clean_up_pipeline(str(x)) for x in data_test]\n",
        "\n",
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynnu2SYm9Fop"
      },
      "source": [
        "2. Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx1EhcP_9MDy",
        "outputId": "f4924c75-bc84-4f44-ab42-a5017308aac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "x_train_features = np.array(tokenizer.texts_to_sequences(X_train))\n",
        "x_test_features = np.array(tokenizer.texts_to_sequences(X_test))\n",
        "\n",
        "len(x_train_features[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBcpAEvvuUXT",
        "outputId": "1767082d-e88b-49a1-eaa1-acbbe81cdd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text               :  subject re languag acquisit analog pragmaticsn n ye inde care star sentenc sentenc star doe mean passerbi one even brentwood sever week ago wit cruel doubl murder testimoni broken english requir servic profession linguist interpret latter wa nt sure onc o j simpson star sentenc result is futur histori jule levin n\n",
            "Numerical Sequence :  [9, 74, 5, 220, 1185, 2275, 2, 319, 1015, 1016, 1412, 589, 589, 1412, 47, 54, 3414, 11, 79, 3415, 250, 107, 343, 1413, 3416, 788, 3417, 3418, 3419, 12, 234, 377, 536, 4, 344, 1017, 10, 34, 272, 452, 171, 172, 3420, 1412, 589, 273, 1, 789, 537, 2276, 3421, 2]\n",
            "Text               :  subject human sens disambigu correctionn n oh dear first send summari late discov summari ha error second refer ve read g miller commun annual review psycholog vol page sorri mail mark sanderson depart comput scienc univers glasgow g qq scotland uk e mail sanderso dc glasgow ac uk tel x number fax m gonna tent tent tent tent tent both experi ment ment ment ment n\n",
            "Numerical Sequence :  [9, 136, 345, 2277, 3422, 2, 3423, 538, 62, 39, 378, 893, 590, 378, 13, 713, 147, 48, 320, 195, 154, 1751, 221, 2278, 409, 790, 645, 791, 1186, 43, 290, 3424, 21, 114, 55, 8, 1018, 154, 2279, 894, 137, 20, 43, 3425, 3426, 1018, 155, 137, 410, 1019, 49, 50, 22, 3427, 1020, 1020, 1020, 1020, 1020, 82, 129, 1187, 1187, 1187, 1187, 2]\n",
            "Text               :  subject summari doctor program appli linguisticsn n sever week ago post queri doctor program appli linguist receiv respons thank respond respons are organ below note were respons georgetown ucla though were mention one respons michael toolan toolan u washington edu nt entir separ program appli linguist within our english dept provid concentr cours languag discours studi lead ph d re particularli equip supervis dissert rhetor composit theori b discours analysi among student take option are one complet our ma tesol michael toolan dept english u washington seattl robert port port cs indiana edu depend mean appli linguist indiana univers ha program area is focuss especi teach english second languag offer train topic lexicographi creol sociolinguist etc want phd linguist request inform depart linguist memori hall indiana univers bloomington lingdept indiana edu stanley dubinski dubinsk univscvm csd scarolina edu greet friend might want consid appli linguist program here univers south carolina linguist programher is interdepartment program involv core faculti seven dept includ english french spanish anthro phil psych speech path graduat student most whom are special sla theori esl popular special here includ gener sociolinguist discours convers analysi codeswitch dialect studi are three thing might program attract friend besid winter weather lack thereof involv faculti divers depart creat wealth potenti avenu research our student ii despit interdisciplinari natur program requir strong emphasi core area theoret linguist iii hire sla theoretician program ha question pleas answer best stan dubinski stanley dubinski e mail dubinsk univscvm csd scarolina edu linguist program phone u south carolina fax columbia sc one anonym respond suggest uc berkeley ha use flexibl system wherebi student creat own program person suggest penn state educ linguist univers hawai mano sarah g thomason salli isp pitt edu pitt ha specialti appli linguist within ph d program linguist student littl interest linguist per se are better off appli linguist ph d program are quit few ucla hawaii most promin carnegi mellon univers announc one faculti is small financi aid is like gather nativelik fluenci french german spanish japanes usc southern calif ha ph d program appli ling wrong mayb s our linguist special track app ling likewis u delawar salli alan juff juff isp pitt edu univers pittsburgh offer phd appli linguist believ is strong program sinc give student thorough train linguist opportun actual work appli area student are current fund through english languag institut variou research project relat languag teach appli program is particularli strong field second languag acquisit both ug orient cognit approach addit offer cours languag plan sociolinguist languag contact pleas nt hesit write further inform alan juff admiss offic alan juff tel dept linguist fax cl email juff isp pitt edu univers pittsburgh pittsburgh pa usa marina mcintir mmcintir lynx dac neu edu program bu appli linguist is pretti one one thing s asl connect is strong one s pretti far theoret side still ricki jacob hi susi sic voic past program here is direct counterpart appli program ucla is call polit reason ph d sla though cover much s hous esl here ha quit few recogn star detail check professor charlen sato chair ph d program sla depart english second languag univers hawai manoa east west road honolulu hi ali aghbar aaghbar grove iup edu ph d program rhetor linguist student choos concentr composit tesol enrol round summer inform contact director graduat program rhetor linguist depart english indiana univers pa indiana pa susan fischer internet sdfncr rit edu nation technic institut deaf phone rochest institut technolog fax lomb memori drive basic food group popcorn rochest ny tofu bok choy chocolaten\n",
            "Numerical Sequence :  [9, 378, 1021, 26, 85, 1188, 2, 250, 107, 343, 101, 115, 1021, 26, 85, 4, 60, 90, 61, 222, 90, 6, 379, 274, 182, 97, 90, 3428, 499, 411, 97, 251, 11, 90, 380, 1752, 1752, 86, 714, 44, 34, 1022, 792, 26, 85, 4, 206, 18, 12, 346, 148, 1753, 68, 5, 275, 45, 1414, 347, 29, 74, 453, 2280, 1754, 412, 1023, 1189, 42, 116, 275, 121, 591, 31, 500, 1755, 6, 11, 207, 18, 715, 1415, 380, 1752, 346, 12, 86, 714, 1756, 1190, 2281, 2281, 1191, 793, 44, 381, 54, 85, 4, 793, 8, 13, 26, 140, 1, 3429, 382, 98, 12, 147, 5, 183, 1024, 501, 2282, 1757, 646, 102, 69, 716, 4, 717, 14, 21, 4, 1416, 895, 793, 8, 3430, 3431, 793, 44, 2283, 1758, 2284, 1759, 1760, 1761, 44, 2285, 383, 138, 69, 348, 85, 4, 26, 64, 8, 896, 1762, 4, 3432, 1, 1763, 26, 349, 1417, 321, 2286, 346, 56, 12, 93, 276, 2287, 1418, 2288, 141, 3433, 304, 31, 75, 1764, 6, 63, 1419, 42, 1025, 3434, 63, 64, 56, 35, 646, 275, 1420, 121, 3435, 130, 45, 6, 291, 117, 138, 26, 897, 383, 1421, 1192, 3436, 1765, 2289, 349, 321, 454, 21, 718, 2290, 592, 1422, 65, 18, 31, 1026, 2291, 2292, 235, 26, 234, 593, 1766, 1417, 140, 384, 4, 2293, 1027, 1419, 3437, 26, 13, 70, 36, 385, 594, 2294, 1758, 2283, 1758, 20, 43, 2284, 1759, 1760, 1761, 44, 4, 26, 184, 86, 896, 1762, 50, 3438, 3439, 11, 1193, 222, 156, 1767, 1028, 13, 27, 2295, 185, 3440, 31, 718, 122, 26, 118, 156, 3441, 131, 413, 4, 8, 1768, 3442, 1769, 154, 3443, 2296, 794, 1423, 44, 1423, 13, 3444, 85, 4, 206, 347, 29, 26, 4, 31, 58, 25, 4, 305, 647, 6, 1029, 1030, 85, 4, 347, 29, 26, 6, 322, 196, 499, 2297, 75, 2298, 3445, 3446, 8, 455, 11, 321, 1, 414, 898, 1031, 1, 1194, 2299, 3447, 1770, 93, 323, 276, 1032, 1424, 719, 3448, 13, 347, 29, 26, 85, 899, 1033, 795, 3, 18, 4, 63, 1195, 1771, 899, 1772, 86, 3449, 2296, 1034, 1196, 1196, 794, 1423, 44, 8, 1773, 183, 716, 85, 4, 456, 1, 593, 26, 119, 502, 31, 2300, 1024, 4, 186, 539, 19, 85, 140, 31, 6, 187, 415, 197, 12, 5, 252, 324, 65, 253, 108, 5, 98, 85, 26, 1, 453, 593, 254, 147, 5, 220, 82, 3450, 416, 648, 325, 208, 183, 68, 5, 796, 646, 5, 173, 36, 34, 2301, 162, 157, 14, 1034, 1196, 2302, 649, 1034, 1196, 410, 346, 4, 50, 2303, 17, 1196, 794, 1423, 44, 8, 1773, 1773, 650, 540, 3451, 3452, 3453, 2304, 3454, 2305, 44, 26, 1774, 85, 4, 1, 1197, 11, 11, 117, 3, 1775, 541, 1, 593, 11, 3, 1197, 306, 384, 797, 198, 3455, 3456, 1198, 3457, 3458, 1035, 503, 26, 64, 1, 386, 3459, 85, 26, 499, 1, 163, 109, 236, 347, 29, 1419, 411, 720, 83, 3, 900, 1025, 64, 13, 322, 196, 1776, 1412, 387, 388, 504, 3460, 3461, 721, 347, 29, 26, 1419, 21, 12, 147, 5, 8, 1768, 3462, 1199, 1036, 1425, 2306, 1198, 1777, 3463, 3464, 3465, 3466, 44, 347, 29, 26, 1023, 4, 31, 1200, 1753, 1189, 1415, 2307, 1778, 901, 14, 173, 1037, 304, 26, 1023, 4, 21, 12, 793, 8, 650, 793, 650, 2308, 3467, 255, 3468, 3469, 44, 350, 307, 252, 2309, 184, 1426, 252, 595, 50, 3470, 1416, 798, 351, 1779, 256, 3471, 1426, 1038, 3472, 3473, 3474, 3475]\n",
            "Text               :  subject oulipon n am look inform oulipo experiment languag group flourish s pari member includ r queneau h matthew calvino g perec group appli mathemat method write literatur is scant inform avail english small amount french am particularli interest whether idea been embodi softwar literati theori thank michael sikillian annotextn\n",
            "Numerical Sequence :  [9, 3476, 2, 30, 149, 14, 3477, 2310, 5, 256, 3478, 3, 1427, 158, 56, 123, 3479, 209, 1780, 3480, 154, 3481, 256, 85, 1428, 651, 162, 188, 1, 3482, 14, 124, 12, 414, 722, 93, 30, 453, 25, 110, 292, 33, 3483, 257, 3484, 42, 61, 380, 2311, 2312]\n",
            "Text               :  subject re polit correctnessn n term polit correct togeth polit correct pc are be use increas frequenc did nexi search coupl week ago connect present m do ncte next week shift mean term while pc still retain neg leftw iron slant is come use broadli attack partylin extremist stripe broadli still often without appar neg iron connot refer do right thing expect thing appro priat thing context exampl appear busi week info world nonpolit context articl salari level corpor ceo appropri valu return write line comput program code one nice cite note while wa easi send email boss clerk mailroom wa necessarili polit correct wonder often send email mailroom clerk denni debaron uiuc edu fax denni baron depart english univ illinoi s wright st urbana il n\n",
            "Numerical Sequence :  [9, 74, 109, 3485, 2, 84, 109, 150, 505, 109, 150, 596, 6, 125, 27, 1429, 902, 293, 3486, 723, 1430, 107, 343, 541, 189, 22, 352, 3487, 294, 107, 1431, 54, 84, 277, 596, 198, 2313, 597, 2314, 1039, 3488, 1, 724, 27, 1781, 2315, 3489, 3490, 3491, 1781, 198, 652, 258, 903, 597, 1039, 2316, 48, 352, 159, 117, 542, 117, 3492, 3493, 117, 506, 80, 598, 190, 107, 725, 126, 3494, 506, 389, 653, 120, 1432, 3495, 654, 655, 904, 162, 417, 114, 26, 507, 11, 1782, 905, 182, 277, 10, 508, 39, 17, 2317, 2318, 2319, 10, 1201, 109, 150, 1202, 652, 39, 17, 2319, 2318, 2320, 3496, 1783, 44, 50, 2320, 3497, 21, 12, 2321, 1203, 3, 1433, 543, 2322, 1434, 2]\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(\"Text               : \",X_train[i] )\n",
        "    print(\"Numerical Sequence : \", x_train_features[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW6vS2XnpD3s"
      },
      "outputs": [],
      "source": [
        "tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBC8HD5ZrREf",
        "outputId": "965eec46-da43-497d-fdf4-2e333bccf1d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(label_train)\n",
        "y_test = le.transform(label_test)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKDk6HY8tzZf",
        "outputId": "0b14c720-156a-4457-86f0-7f6f18403676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_feature 7016\n",
            "max_len 2277\n"
          ]
        }
      ],
      "source": [
        "embed_size = 20 # how big is each word vector\n",
        "max_feature = len(tokenizer.word_index)+1\n",
        "max_len = max_length_sequence = max([len(i) for i in x_train_features])\n",
        "print(\"max_feature\", max_feature)\n",
        "print(\"max_len\", max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMoQTyA_u_sL",
        "outputId": "2f068ddd-3744-42a1-8c3b-5046e7f41af4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0, ..., 2276, 3421,    2], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "x_train_features = pad_sequences(x_train_features,maxlen=max_len)\n",
        "x_test_features = pad_sequences(x_test_features,maxlen=max_len)\n",
        "x_train_features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8m6TtN9Y0U"
      },
      "source": [
        "4.Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "lycZmWxal4me"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional\n",
        "from keras.models import Model\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8iATSKIjtrx",
        "outputId": "2bc27b0a-ca6a-454f-b0be-38f0c57802db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 2277, 32)          224512    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                24832     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 250,665\n",
            "Trainable params: 250,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "    \n",
        "      lstm_model = tf.keras.Sequential()\n",
        "      lstm_model.add(Embedding(max_feature, 32, input_length=max_len))\n",
        "      lstm_model.add(LSTM(64))\n",
        "      lstm_model.add(Dropout(0.5))\n",
        "      lstm_model.add(Dense(20, activation=\"relu\"))\n",
        "      lstm_model.add(Dropout(0.4))\n",
        "      lstm_model.add(Dense(1, activation = \"sigmoid\"))\n",
        "      return lstm_model\n",
        "\n",
        "lstm_model = create_model()\n",
        "lstm_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOD8JT-Yqrv3",
        "outputId": "a84ef2fa-1e96-419a-aeb9-0298d7c98e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/14\n",
            "11/11 [==============================] - 3s 132ms/step - loss: 0.6592 - accuracy: 0.9012 - val_loss: 0.6075 - val_accuracy: 0.8776\n",
            "Epoch 2/14\n",
            "11/11 [==============================] - 1s 81ms/step - loss: 0.4253 - accuracy: 0.9259 - val_loss: 0.5240 - val_accuracy: 0.8776\n",
            "Epoch 3/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.3450 - accuracy: 0.9259 - val_loss: 0.4579 - val_accuracy: 0.8776\n",
            "Epoch 4/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.2687 - accuracy: 0.9259 - val_loss: 0.4001 - val_accuracy: 0.8776\n",
            "Epoch 5/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.2944 - accuracy: 0.9259 - val_loss: 0.3763 - val_accuracy: 0.8776\n",
            "Epoch 6/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.2882 - accuracy: 0.9259 - val_loss: 0.3614 - val_accuracy: 0.8776\n",
            "Epoch 7/14\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 0.2900 - accuracy: 0.9259 - val_loss: 0.3571 - val_accuracy: 0.8776\n",
            "Epoch 8/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.2380 - accuracy: 0.9259 - val_loss: 0.3076 - val_accuracy: 0.8776\n",
            "Epoch 9/14\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 0.1682 - accuracy: 0.9321 - val_loss: 0.2837 - val_accuracy: 0.8776\n",
            "Epoch 10/14\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 0.1077 - accuracy: 0.9691 - val_loss: 0.1329 - val_accuracy: 0.9184\n",
            "Epoch 11/14\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 0.0576 - accuracy: 1.0000 - val_loss: 0.1366 - val_accuracy: 0.9388\n",
            "Epoch 12/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.0464 - accuracy: 0.9877 - val_loss: 0.1481 - val_accuracy: 0.9388\n",
            "Epoch 13/14\n",
            "11/11 [==============================] - 1s 82ms/step - loss: 0.0486 - accuracy: 0.9938 - val_loss: 0.0831 - val_accuracy: 1.0000\n",
            "Epoch 14/14\n",
            "11/11 [==============================] - 1s 84ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9796\n"
          ]
        }
      ],
      "source": [
        "his = lstm_model.fit(x_train_features, y_train, batch_size=16, epochs=14, validation_data=(x_test_features, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save('model_0.9796.h5')"
      ],
      "metadata": {
        "id": "nlBKY1TqqRAM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict  = [1 if x>=0.5 else 0 for x in lstm_model.predict(x_test_features)]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test,y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gygq04ej70SJ",
        "outputId": "ed94113b-2860-4ea6-a013-130cb0ab1e5e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 53ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.83      0.91         6\n",
            "           1       0.98      1.00      0.99        43\n",
            "\n",
            "    accuracy                           0.98        49\n",
            "   macro avg       0.99      0.92      0.95        49\n",
            "weighted avg       0.98      0.98      0.98        49\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWt1QffqOl6X"
      },
      "source": [
        "label on test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr0t_KO3OhiM"
      },
      "outputs": [],
      "source": [
        "data_not_spam = os.listdir(\"/content/TestData_nolabel/\")\n",
        "\n",
        "test_nolabel = []\n",
        "arr_nolabel=[]\n",
        "for i in data_not_spam:\n",
        "  path = os.path.join(\"/content/TestData_nolabel/\", i)\n",
        "  arr_nolabel.append(i)\n",
        "  with open(path, 'r') as f:\n",
        "    test_nolabel.append(f.readlines())\n",
        "\n",
        "len(test_nolabel)\n",
        "arr_nolabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "y-hWAMhUO81w",
        "outputId": "99c63ba4-c55a-4b56-e9ca-c0cf91287e33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'subject black hole pre chomskyan citationsn n steve anderson s book morpholog contain follow ing epigraph did nt chang befor public linguist becom scienc linguist begin stand one anoth s shoulder instead one anoth s toe ha point reach point where are redo aspect languag poorli were done first problem origin fact littl had been done syntax prior work gener school littl is comparison ha been done sinc instig movement jakobson s hall s work distinct featur clearli superced pre viou work make difficult structuralist work relev is go today howev anderson is right chide us carri attitud over morpholog where current trend around massachusett ha hardli move beyond bloomfield first claim affix are regular lexic item first rate morpholog cal studi goe back stoic philosoph were first teas apart grammat categori differ track back panini is most current morpholog fail cite relev sourc is fail advantag discoveri struc turalist neogrammarian even classic morphologist pre decessor were particularli adept find problem theori linguist sign varro wa first attempt defin lexic categori term n v lexic aristotl notic grammat morphem differ lexic one stoic first use term signifi signifi am jump middl discuss mark ha touch real issu is less chomski member school are quot much mani other contri bution often same one are quot littl result am see often is second third fourth reinvent wheel rbeardn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "arr_test = [clean_up_pipeline(str(x)) for x in test_nolabel]\n",
        "\n",
        "arr_test[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgsMdQQvPctv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# tokenizer = Tokenizer(num_words=max_feature)\n",
        "\n",
        "# tokenizer.fit_on_texts(arr_test)\n",
        "\n",
        "arr_test_feature = np.array(tokenizer.texts_to_sequences(arr_test))\n",
        "\n",
        "arr_test_feature[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWv25_PXPyfV",
        "outputId": "1598aac1-79c9-4b96-d2a1-6b873be8f284"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0, ...,  406, 2443, 2444], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "arr_test_feature = pad_sequences(arr_test_feature,maxlen=max_len)\n",
        "\n",
        "arr_test_feature[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49BOXexuOeVu",
        "outputId": "efc67e93-336b-449b-ca84-5ccd0e6ccf3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 58ms/step\n"
          ]
        }
      ],
      "source": [
        "y_predict_label  = [1 if x>=0.5 else 0 for x in lstm_model.predict(arr_test_feature)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60nt5pfLP_Cu",
        "outputId": "d5de6711-0801-43b2-f230-f34782c62448"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "y_predict_label.count(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5bzUvHh04Fy",
        "outputId": "ee939ed1-b1ff-43cb-b1a1-c2043a402760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File written successfully\n"
          ]
        }
      ],
      "source": [
        "with open('/content/label_predict.txt', 'w+') as f:\n",
        "     \n",
        "    # write elements of list\n",
        "    for items in y_predict_label:\n",
        "        f.write('%s\\n' %items)\n",
        "     \n",
        "    print(\"File written successfully\")\n",
        " \n",
        " \n",
        "# close the file\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_label=list()\n",
        "with open('/content/label_predict.txt', 'r') as f:\n",
        "  arr_label.append(f.readlines())\n",
        "arr_label = np.reshape(arr_label,(78))\n",
        "arr_la = []\n",
        "for i in arr_label:\n",
        "  arr_la.append(int(i[0:1]))\n",
        "arr_la = [\"notspam\" if x==1 else \"spam\" for x in arr_la]\n",
        "len(arr_la)"
      ],
      "metadata": {
        "id": "0pcqXkDXuXae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04554613-fbec-4a0f-d45e-0d8bb2152be7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(78):\n",
        "  with open('/content/predict_0.9796.txt', 'w+') as f:\n",
        "     \n",
        "    # write elements of list\n",
        "    for i in range(78):\n",
        "        f.write('%s' % arr_nolabel[i])\n",
        "        f.write(\",\")\n",
        "        f.write('%s' % arr_la[i])\n",
        "        f.write('\\n')\n",
        "\n",
        "     \n",
        "print(\"File written successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4jDovBv3VdU",
        "outputId": "4f1fdec2-7762-46e5-8230-baa044e0fdad"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File written successfully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}